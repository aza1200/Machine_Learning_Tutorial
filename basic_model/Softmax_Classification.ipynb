{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=jMU9G5WEtBc&list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm&index=15\n",
    "# 미분식 : https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (   0/1000) cost: 0.01824, W: tensor([[-0.0665, -0.0736,  0.1402]]), b:0.0\n",
      "Epoch ( 100/1000) cost: 0.006376, W: tensor([[-0.0906, -0.0778,  0.1683]]), b:-0.0\n",
      "Epoch ( 200/1000) cost: 0.006376, W: tensor([[-0.0906, -0.0778,  0.1683]]), b:-1e-07\n",
      "Epoch ( 300/1000) cost: 0.006376, W: tensor([[-0.0906, -0.0778,  0.1683]]), b:-1e-07\n",
      "Epoch ( 400/1000) cost: 0.006376, W: tensor([[-0.0906, -0.0778,  0.1683]]), b:-2e-07\n",
      "Epoch ( 500/1000) cost: 0.006376, W: tensor([[-0.0906, -0.0778,  0.1683]]), b:-2e-07\n",
      "Epoch ( 600/1000) cost: 0.006376, W: tensor([[-0.0906, -0.0778,  0.1683]]), b:-3e-07\n",
      "Epoch ( 700/1000) cost: 0.006376, W: tensor([[-0.0906, -0.0778,  0.1683]]), b:-3e-07\n",
      "Epoch ( 800/1000) cost: 0.006376, W: tensor([[-0.0906, -0.0778,  0.1683]]), b:-4e-07\n",
      "Epoch ( 900/1000) cost: 0.006376, W: tensor([[-0.0906, -0.0778,  0.1683]]), b:-4e-07\n",
      "W: tensor([[-0.0906, -0.0778,  0.1683]])\n",
      "b: tensor([-4.4441e-07])\n",
      "result : \n",
      "tensor([[0.3023, 0.3062, 0.3916],\n",
      "        [0.2700, 0.2770, 0.4531],\n",
      "        [0.2373, 0.2466, 0.5160],\n",
      "        [0.0516, 0.0593, 0.8891],\n",
      "        [0.0408, 0.0476, 0.9116],\n",
      "        [0.0321, 0.0379, 0.9299],\n",
      "        [0.0043, 0.0056, 0.9900],\n",
      "        [0.0033, 0.0044, 0.9922],\n",
      "        [0.0026, 0.0035, 0.9940]])\n"
     ]
    }
   ],
   "source": [
    "# 1. 직접 미분해서 구해보기\n",
    "# 시그모이드는 사용안된다 \n",
    "# softmax -> log -> 취해줌\n",
    "# log_softmax -> 취해주고 다 더해주고 평균\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n",
    "\n",
    "x_data = [[1],[2],[3],[11],[12],[13],[21],[22],[23]]\n",
    "y_data = [[0],[0],[0],[1], [1], [1], [2], [2], [2]]\n",
    "\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.LongTensor(y_data)\n",
    "\n",
    "w = torch.zeros((1,3))\n",
    "b = torch.zeros(1)\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(1000):\n",
    "    for tmp_idx in range(len(x_data)):\n",
    "        now_train_x = x_train[tmp_idx]\n",
    "        now_train_y = y_train[tmp_idx]  \n",
    "        \n",
    "        before_softmax = now_train_x@w + b \n",
    "        \n",
    "        # print(before_softmax)\n",
    "        after_softmax =  torch.softmax(before_softmax, dim =0)\n",
    "        after_log = -torch.log(after_softmax)\n",
    "        \n",
    "        # print(f\"x_train : {now_train_x}\")\n",
    "        # print(f\"y_train : {now_train_y}\")\n",
    "        # print(f\"after softmax : {after_softmax}\")\n",
    "        # print(f\"after log : {after_log}\")\n",
    "\n",
    "\n",
    "        hypothesis = after_log\n",
    "        y_one_hot = torch.zeros_like(hypothesis)\n",
    "        y_one_hot.scatter_(0, now_train_y ,1)\n",
    "        \n",
    "        # print(f\"y_one_hot :  {y_one_hot}\")\n",
    "        \n",
    "        cost = (y_one_hot * after_log).sum()\n",
    "        loss_derivate_wrt_to_z = after_softmax - y_one_hot\n",
    "        # print(f\"loss_derivate : {loss_derivate_wrt_to_z}\")\n",
    "        \n",
    "        gradient_w = (loss_derivate_wrt_to_z) * now_train_x\n",
    "        gradient_b = loss_derivate_wrt_to_z.mean()\n",
    "        # print(f\"gradient_w : {gradient_w}\")\n",
    "        # print(f\"gradient_b : {gradient_b}\")        \n",
    "        \n",
    "        w -= learning_rate * gradient_w\n",
    "        b -= learning_rate * gradient_b\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch ({str(epoch).rjust(4)}/1000) cost: {round(float(cost),6)}, W: {w}, b:{round(float(b),7)}')\n",
    "    \n",
    "\n",
    "print(f'W: {w}')\n",
    "print(f'b: {b}')\n",
    "print('result : ')\n",
    "print(torch.softmax(x_train@w + b,dim=1))    \n",
    "\n",
    "# 결과\n",
    "# 가장 초기버전 \n",
    "# wx + b 로는 표현하기 좀 힘든거같음\n",
    "# 직접 미분, SGD 로는 조금 불안정한걱 같음 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000, Loss: 1.1870\n",
      "Epoch 100/1000, Loss: 0.2698\n",
      "Epoch 200/1000, Loss: 0.0833\n",
      "Epoch 300/1000, Loss: 0.0366\n",
      "Epoch 400/1000, Loss: 0.0219\n",
      "Epoch 500/1000, Loss: 0.0152\n",
      "Epoch 600/1000, Loss: 0.0113\n",
      "Epoch 700/1000, Loss: 0.0088\n",
      "Epoch 800/1000, Loss: 0.0071\n",
      "Epoch 900/1000, Loss: 0.0059\n",
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Chat GPT 가 내놓은 대답....\n",
    "# 미친 거 같음 \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the model\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Prepare the data\n",
    "x_data = [[1], [2], [3], [11], [12], [13], [21], [22], [23]]\n",
    "y_data = [0, 0, 0, 1, 1, 1, 2, 2, 2]\n",
    "\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.LongTensor(y_data)\n",
    "\n",
    "# Define the model, loss function, and optimizer\n",
    "model = MyModel()\n",
    "\n",
    "## 요 오차 구해주는 함수는 오차 평균값으로 계산해줌\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 1000\n",
    "batch_size = 9\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle the data before each epoch\n",
    "    indices = torch.randperm(x_train.shape[0])\n",
    "    x_train = x_train[indices]\n",
    "    y_train = y_train[indices]\n",
    "\n",
    "    for i in range(0, len(x_data), batch_size):\n",
    "        # Forward pass\n",
    "        inputs = x_train[i:i+batch_size]\n",
    "        labels = y_train[i:i+batch_size]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "# model.eval() 효과\n",
    "\n",
    "# 1. dropout 이 임의로 몇개 input 0으로 바꾸는데 이과정 없앰 \n",
    "# 2. batch normalization\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(x_train)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = (predicted == y_train).sum().item() / len(y_train)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
