{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (         0/      1000) cost: 102.904395, W:   1.070070, b:  0.681684\n",
      "Epoch (       100/      1000) cost:   0.107595, W:   2.825976, b:  0.745032\n",
      "Epoch (       200/      1000) cost:   0.051788, W:   2.879267, b:  0.516883\n",
      "Epoch (       300/      1000) cost:   0.024926, W:   2.916239, b:  0.358599\n",
      "Epoch (       400/      1000) cost:   0.011998, W:   2.941889, b:  0.248786\n",
      "Epoch (       500/      1000) cost:   0.005775, W:   2.959684, b:  0.172601\n",
      "Epoch (       600/      1000) cost:   0.002779, W:   2.972030, b:  0.119746\n",
      "Epoch (       700/      1000) cost:   0.001338, W:   2.980595, b:  0.083076\n",
      "Epoch (       800/      1000) cost:   0.000644, W:   2.986537, b:  0.057636\n",
      "Epoch (       900/      1000) cost:   0.000310, W:   2.990660, b:  0.039986\n",
      "W:   2.993496\n",
      "b:   0.027843\n",
      "result : \n",
      "[ 3.02133943  6.01483589  9.00833234 12.00182879 14.99532524 17.98882169]\n"
     ]
    }
   ],
   "source": [
    "# 1. numpy 로 gradient descent 구현\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "xy = np.array([\n",
    "    [1.,2.,3.,4.,5.,6.,], # y = 3x\n",
    "    [3.,6.,9.,12.,15.,18.],\n",
    "    ])\n",
    "\n",
    "x_train = xy[0]\n",
    "y_train = xy[1]\n",
    "\n",
    "beta_gd = np.random.rand(1) # y= random한숫자 곱하기 x\n",
    "bias = np.random.rand(1)  # y= ax +b  \n",
    "\n",
    "learning_rate = 0.01\n",
    "n_data = len(x_train)\n",
    "\n",
    "for i in range(1000):\n",
    "    hypothesis = x_train * beta_gd + bias\n",
    "    cost = np.sum( ((hypothesis - y_train)**2)/ n_data)\n",
    "    \n",
    "    gradient_w = np.sum((beta_gd * x_train - y_train + bias) * 2 * x_train) / n_data\n",
    "    gradient_b = np.sum((beta_gd * x_train - y_train + bias) * 2) / n_data\n",
    "    \n",
    "    beta_gd -= learning_rate * gradient_w\n",
    "    bias -= learning_rate * gradient_b\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print('Epoch ({:10d}/{:10d}) cost: {:10f}, W: {:10f}, b:{:10f}'.format(i, 1000, cost, float(beta_gd), float(bias)))\n",
    "        \n",
    "print('W: {:10f}'.format(float(beta_gd)))\n",
    "print('b: {:10f}'.format(float(bias)))\n",
    "print('result : ')\n",
    "print(x_train * beta_gd + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (   0/1000) cost: 52.476868, W: 1.5300145, b:0.8875664\n",
      "Epoch ( 100/1000) cost: 0.127115, W: 2.8108475, b:0.8097996\n",
      "Epoch ( 200/1000) cost: 0.061183, W: 2.8687713, b:0.5618165\n",
      "Epoch ( 300/1000) cost: 0.029449, W: 2.9089572, b:0.3897727\n",
      "Epoch ( 400/1000) cost: 0.014174, W: 2.936837, b:0.2704135\n",
      "Epoch ( 500/1000) cost: 0.006822, W: 2.9561794, b:0.1876054\n",
      "Epoch ( 600/1000) cost: 0.003284, W: 2.9695985, b:0.1301552\n",
      "Epoch ( 700/1000) cost: 0.00158, W: 2.9789081, b:0.090298\n",
      "Epoch ( 800/1000) cost: 0.000761, W: 2.9853673, b:0.0626462\n",
      "Epoch ( 900/1000) cost: 0.000366, W: 2.9898481, b:0.0434624\n",
      "Epoch (1000/1000) cost: 0.000176, W: 2.9929569, b:0.030153\n",
      "W:   2.992957\n",
      "b:   0.030153\n",
      "result : \n",
      "tensor([ 3.0231,  6.0161,  9.0090, 12.0020, 14.9949, 17.9879],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 2. pytorch auto_grad 로 구현\n",
    "# 참조 사이트 : https://machinelearningmastery.com/implementing-gradient-descent-in-pytorch/\n",
    "import torch\n",
    "\n",
    "x = torch.tensor([1.,2.,3.,4.,5.,6.])\n",
    "y = torch.tensor([3.,6.,9.,12.,15.,18.])\n",
    "\n",
    "w = torch.rand(1, requires_grad=True) # computation_graph 를 생성하기 위해서\n",
    "b = torch.rand(1, requires_grad=True) # requires_grad = True 로 설정하고 \n",
    "learning_rate = 0.01                  # 설정시, 미분값을 자동적으로 저장하게 된다.\n",
    "                                      # w,b 는 leaf node 이다.              \n",
    "for epoch in range(1001):\n",
    "    y_hat = x * w + b\n",
    "    loss = ((y_hat - y)**2).mean()\n",
    "    \n",
    "    w.retain_grad() # retain_grad 를 하지 않을시 non-leaf 들은 .grad 속성이 None으로저장됨\n",
    "    b.retain_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    w = w - learning_rate * w.grad\n",
    "    b = b - learning_rate * b.grad\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch ({str(epoch).rjust(4)}/1000) cost: {round(float(loss),6)}, W: {round(float(w),7)}, b:{round(float(b),7)}')\n",
    "        \n",
    "print('W: {:10f}'.format(float(w)))\n",
    "print('b: {:10f}'.format(float(b)))\n",
    "print('result : ')\n",
    "print(x * w + b)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model : MyNeuralNetwork(\n",
      "  (layer): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "weight : Parameter containing:\n",
      "tensor([[0.3685]], requires_grad=True)\n",
      "bias : Parameter containing:\n",
      "tensor([0.0889], requires_grad=True)\n",
      "Epoch (   0/1000) cost: 103.39817, W: 1.1604834, b:0.2713251\n",
      "Epoch ( 100/1000) cost: 0.041226, W: 2.8922796, b:0.4611719\n",
      "Epoch ( 200/1000) cost: 0.019843, W: 2.9252665, b:0.3199482\n",
      "Epoch ( 300/1000) cost: 0.009551, W: 2.9481521, b:0.2219715\n",
      "Epoch ( 400/1000) cost: 0.004597, W: 2.9640293, b:0.1539977\n",
      "Epoch ( 500/1000) cost: 0.002213, W: 2.9750445, b:0.1068392\n",
      "Epoch ( 600/1000) cost: 0.001065, W: 2.9826868, b:0.0741222\n",
      "Epoch ( 700/1000) cost: 0.000513, W: 2.9879885, b:0.0514237\n",
      "Epoch ( 800/1000) cost: 0.000247, W: 2.9916668, b:0.0356764\n",
      "Epoch ( 900/1000) cost: 0.000119, W: 2.9942186, b:0.0247512\n",
      "Epoch (1000/1000) cost: 5.7e-05, W: 2.9959891, b:0.0171719\n",
      "W:   2.995989\n",
      "b:   0.017172\n",
      "result : \n",
      "tensor([[ 3.0132],\n",
      "        [ 6.0092],\n",
      "        [ 9.0051],\n",
      "        [12.0011],\n",
      "        [14.9971],\n",
      "        [17.9931]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 3. Neural Network 로 gradient_descent 구현\n",
    "\n",
    "# 클래스 정의하기\n",
    "# 신경망 모델을 nn.Module 의 하위클래스로 정의하고,\n",
    "# __init__ 에서 신경망 계층들을 초기화합니다. \n",
    "# nn.Module 을 상속받은 모든 클래스는 forward 메소드에\n",
    "# 입력 데이터에 대한 연산들을 구현합니다.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 신경망 정의\n",
    "class MyNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNeuralNetwork, self).__init__()\n",
    "        self.layer=nn.Linear(in_features=1, out_features=1, bias=True)\n",
    "        \n",
    "        # 아래 네줄은 써도 상관없고 안써도 상관없다. 같은 결과를 내뱉음 \n",
    "        # weight = torch.rand(1).view(1,1) # 2d matrix 로 초기화\n",
    "        # bias = torch.rand(1).view(1,1)\n",
    "        # self.layer.weight = nn.Parameter(weight)\n",
    "        # self.layer.bias = nn.Parameter(bias)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = self.layer(input)\n",
    "        return output\n",
    "    \n",
    "model = MyNeuralNetwork().to(\"cpu\")\n",
    "\n",
    "print(f\"model : {model}\")\n",
    "print(f\"weight : {model.layer.weight}\")\n",
    "print(f\"bias : {model.layer.bias}\")\n",
    "\n",
    "input = torch.tensor([1.,2.,3.,4.,5.,6.,]).view(-1,1)\n",
    "y = torch.tensor([3.,6.,9.,12.,15.,18.]).view(-1,1)\n",
    "\n",
    "\n",
    "############################## 학습률 -> 0.01\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.01)\n",
    "\n",
    "epochs = 1001\n",
    "loss_fn = nn.MSELoss() # 파이토치 에서 구현되있고 제곱빼는 Mean Squared Error \n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    optimizer.zero_grad() # 이전epoch시의 미분값 삭제해버림 \n",
    "    loss = loss_fn(model(input), y)\n",
    "    loss.backward()  # backward : w, b에 대한 기울기 진행\n",
    "    \n",
    "    optimizer.step() # model.parameters() 에서 리턴되는 변수들의 기울기에\n",
    "                     # 학습률 0.01 을 곱해서 빼준뒤에 업데이트 한다.\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch ({str(epoch).rjust(4)}/1000) cost: {round(float(loss),6)}, W: {round(float(model.layer.weight),7)}, b:{round(float(model.layer.bias),7)}')\n",
    "\n",
    "print('W: {:10f}'.format(float(model.layer.weight)))\n",
    "print('b: {:10f}'.format(float(model.layer.bias)))\n",
    "print('result : ')\n",
    "print(model(input))    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
