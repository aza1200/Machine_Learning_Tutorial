{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary_classification 문제입니다.\n",
    "# https://www.youtube.com/watch?v=PIjno6paszY&list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm&index=11\n",
    "\n",
    "# Classification\n",
    "\n",
    "# Regression 은 숫자를 예측하는거라면\n",
    "# Binary Classification 두개중에 하나를 고르는 거임!\n",
    "# 스팸이냐? 혹은 햄?\n",
    "# Facebook Show or Hide? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification 알고리즘 어떻게 적용?\n",
    "# linear regression 으로도 가능할거같다.\n",
    "# 대략 0.5 되는 점 잡아서 크면 통과 작으면 불합격\n",
    "\n",
    "# 예측하면 될것같다 \n",
    "# 실제로 linear 쓸것이다 근데 문제가 있다.\n",
    "# 근데 50시간 통과막 하면 1을 넘어가는 값이 나온다 \n",
    "# 50 을 추가할시 선이 (0.5) 이상 이하의 직선의 기준이\n",
    "# 달라진다.\n",
    "\n",
    "# 그래서 linear Regression 은 간단하기는 하지만\n",
    "# 0 과 1사이로 압축을 시켜주는 함수 하나 있으면 좋겠다.\n",
    "\n",
    "# Sigmoid 를 써서 Cost 를 (y_hat-y) 제곱 형태로\n",
    "# 경사 타면 이차함수 꼴이 안나오기에 새로운 cost 정의 필요\n",
    "\n",
    "# https://www.youtube.com/watch?v=6vzchGYEJBc&list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm&index=12\n",
    "\n",
    "# cost 함수의미? : 실제값과 예측값이 비슷하면 cost 값은 작아지고\n",
    "# 틀리면 cost 를 크게 해서 모델에 벌을 주는 것으로 의미\n",
    "\n",
    "# -log(H(x))   => y=1 일때\n",
    "# -log(1-H(x)) => y=0 일때\n",
    "\n",
    "# 왜? exponential 과 상극인 log 사용\n",
    "# -log(x) 함수는 0일떄 커지고 1일떄 작아짐\n",
    "# H(x) 는 어떤 일이 일어날 확률\n",
    "\n",
    "# -log(x) 와 -log(1-x)를 0과1사이에서 더할시\n",
    "# 우리가 좋아하는 경사타고 내려가기 가능!\n",
    "\n",
    "# C(H(x),y) = -ylog(H(x))-(1-y)log(1-H(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (   0/1000) cost: 0.693147, W: 0.0153125, b:0.0\n",
      "Epoch ( 100/1000) cost: 0.492694, W: 0.2598578, b:-0.1834844\n",
      "Epoch ( 200/1000) cost: 0.450955, W: 0.296653, b:-0.3842742\n",
      "Epoch ( 300/1000) cost: 0.414819, W: 0.329512, b:-0.5713641\n",
      "Epoch ( 400/1000) cost: 0.383442, W: 0.3605963, b:-0.7456194\n",
      "Epoch ( 500/1000) cost: 0.356097, W: 0.3900384, b:-0.9082236\n",
      "Epoch ( 600/1000) cost: 0.33216, W: 0.4179401, b:-1.0602956\n",
      "Epoch ( 700/1000) cost: 0.311107, W: 0.444406, b:-1.202866\n",
      "Epoch ( 800/1000) cost: 0.292497, W: 0.4695402, b:-1.3368645\n",
      "Epoch ( 900/1000) cost: 0.275965, W: 0.4934425, b:-1.4631263\n",
      "W:   0.515985\n",
      "b:  -1.581234\n",
      "result : \n",
      "tensor([[0.2563],\n",
      "        [0.3660],\n",
      "        [0.4917],\n",
      "        [0.8198],\n",
      "        [0.8840],\n",
      "        [0.9273],\n",
      "        [0.9728],\n",
      "        [0.2103]])\n"
     ]
    }
   ],
   "source": [
    "# 1. 직접 미분하면서 구해보기\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 5시간 이상 공부한 친구들 시험 합격\n",
    "x_data = [[1],[2],[3],[6],[7],[8],[10],[0.5]]\n",
    "y_data = [[0],[0],[0],[1],[1],[1],[1],[0]]\n",
    "\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)\n",
    "\n",
    "W = torch.zeros((1,1))\n",
    "b = torch.zeros(1)\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "for epoch in range(1000):\n",
    "    hypothesis = torch.sigmoid(x_train@W +b)\n",
    "    cost = -(\n",
    "        y_train * torch.log(hypothesis) + (1-y_train)* torch.log(1-hypothesis)\n",
    "    ).mean()\n",
    "    # 미분시 결과값 : https://smwgood.tistory.com/6\n",
    "    \n",
    "    gradient_w = ((hypothesis - y_train)*x_train).mean()\n",
    "    gradient_b = (hypothesis - y_train).mean()\n",
    "    \n",
    "    W -= learning_rate * gradient_w\n",
    "    b -= learning_rate * gradient_b \n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch ({str(epoch).rjust(4)}/1000) cost: {round(float(cost),6)}, W: {round(float(W),7)}, b:{round(float(b),7)}')\n",
    "       \n",
    "print('W: {:10f}'.format(float(W)))\n",
    "print('b: {:10f}'.format(float(b)))\n",
    "print('result : ')\n",
    "print(torch.sigmoid(x_train * W + b))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (   0/1000) cost: 0.747136, W: 0.230431, b:0.7487779\n",
      "Epoch ( 100/1000) cost: 0.647581, W: 0.1568906, b:0.4660423\n",
      "Epoch ( 200/1000) cost: 0.585123, W: 0.1949894, b:0.2192409\n",
      "Epoch ( 300/1000) cost: 0.530934, W: 0.2329034, b:-0.010258\n",
      "Epoch ( 400/1000) cost: 0.48404, W: 0.2689974, b:-0.2236102\n",
      "Epoch ( 500/1000) cost: 0.443475, W: 0.3032565, b:-0.4219276\n",
      "Epoch ( 600/1000) cost: 0.408332, W: 0.3357261, b:-0.6064179\n",
      "Epoch ( 700/1000) cost: 0.377798, W: 0.3664809, b:-0.7783027\n",
      "Epoch ( 800/1000) cost: 0.351165, W: 0.3956135, b:-0.9387617\n",
      "Epoch ( 900/1000) cost: 0.327831, W: 0.4232261, b:-1.0888975\n",
      "W:   0.449168\n",
      "b:  -1.228356\n",
      "result : \n",
      "tensor([[0.3145],\n",
      "        [0.4182],\n",
      "        [0.5298],\n",
      "        [0.8125],\n",
      "        [0.8717],\n",
      "        [0.9141],\n",
      "        [0.9631],\n",
      "        [0.2682]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 2. pytorch autograd 사용하여 실제값 구하기\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 5시간 이상 공부한 친구들 시험 합격\n",
    "x_data = [[1],[2],[3],[6],[7],[8],[10],[0.5]]\n",
    "y_data = [[0],[0],[0],[1],[1],[1],[1],[0]]\n",
    "\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)\n",
    "\n",
    "w = torch.rand((1,1), requires_grad=True)\n",
    "b = torch.rand(1, requires_grad=True)\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "for epoch in range(1000):\n",
    "    hypothesis = torch.sigmoid(x_train@w +b)\n",
    "    cost = -(\n",
    "        y_train * torch.log(hypothesis) + (1-y_train)* torch.log(1-hypothesis)\n",
    "    ).mean()\n",
    "    \n",
    "    w.retain_grad()\n",
    "    b.retain_grad()\n",
    "    cost.backward()\n",
    "    \n",
    "    w = w - learning_rate * w.grad  # w 는 resassigned 됨\n",
    "    b = b -  learning_rate * b.grad\n",
    "    # 만일 w -= learning_rate * w.grad 했을시 ,\n",
    "    # w 는 수정되므로 ,leaf_node error 가 나오게 된다. \n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch ({str(epoch).rjust(4)}/1000) cost: {round(float(cost),6)}, W: {round(float(w),7)}, b:{round(float(b),7)}')\n",
    "    \n",
    "print('W: {:10f}'.format(float(w)))\n",
    "print('b: {:10f}'.format(float(b)))\n",
    "print('result : ')\n",
    "print(torch.sigmoid(x_train@w +b))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (   0/1000) cost: 2.013475, W: -0.3309584, b:-0.9473956\n",
      "Epoch (1000/1000) cost: 0.210401, W: 0.6076831, b:-2.0532207\n",
      "Epoch (2000/1000) cost: 0.149039, W: 0.7605073, b:-2.8166595\n",
      "Epoch (3000/1000) cost: 0.116821, W: 0.8741115, b:-3.3709104\n",
      "Epoch (4000/1000) cost: 0.096863, W: 0.964787, b:-3.8074448\n",
      "Epoch (5000/1000) cost: 0.083199, W: 1.0404938, b:-4.1687555\n",
      "Epoch (6000/1000) cost: 0.073204, W: 1.1056569, b:-4.4778156\n",
      "Epoch (7000/1000) cost: 0.065542, W: 1.1629813, b:-4.748414\n",
      "Epoch (8000/1000) cost: 0.059461, W: 1.2142402, b:-4.9894757\n",
      "Epoch (9000/1000) cost: 0.054505, W: 1.2606591, b:-5.2071085\n",
      "W:   1.303081\n",
      "b:  -5.405491\n",
      "result : \n",
      "tensor([[0.0163],\n",
      "        [0.0574],\n",
      "        [0.1830],\n",
      "        [0.9178],\n",
      "        [0.9762],\n",
      "        [0.9934],\n",
      "        [0.9995],\n",
      "        [0.0085]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 3. Neural Network 로 gradient_descent 구현\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MyNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        # python3 에서는 MyNeuralNetwork, self 인자 넣나 안넣나 똑같다.\n",
    "        super(MyNeuralNetwork, self).__init__()\n",
    "        self.layer=nn.Linear(in_features=1, out_features=1, bias=True)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = self.layer(input)\n",
    "        output = torch.sigmoid(output)\n",
    "        return output\n",
    "\n",
    "model = MyNeuralNetwork().to(\"cpu\")\n",
    "\n",
    "input = torch.tensor([[1],[2],[3],[6],[7],[8],[10],[0.5]])\n",
    "y = torch.tensor([[0],[0],[0],[1],[1],[1],[1],[0]], dtype=torch.float)\n",
    "\n",
    "# model.parameter 에는  w, b 가 있으며 SGD 로 학습시킬 계획이다.\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 1000\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "for epoch in range(10000):\n",
    "    \n",
    "    # retain_grad와 \n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_fn(model(input), y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 1000 == 0:\n",
    "         print(f'Epoch ({str(epoch).rjust(4)}/1000) cost: {round(float(loss),6)}, W: {round(float(model.layer.weight),7)}, b:{round(float(model.layer.bias),7)}')\n",
    "\n",
    "\n",
    "print('W: {:10f}'.format(float(model.layer.weight)))\n",
    "print('b: {:10f}'.format(float(model.layer.bias)))\n",
    "print('result : ')\n",
    "print(model(input))    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
