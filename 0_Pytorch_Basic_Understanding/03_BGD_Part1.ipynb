{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (         0/      1000) cost: 127.179154, W:   0.881821, b:  0.639048\n",
      "Epoch (       100/      1000) cost:   0.107849, W:   2.825771, b:  0.745910\n",
      "Epoch (       200/      1000) cost:   0.051910, W:   2.879124, b:  0.517492\n",
      "Epoch (       300/      1000) cost:   0.024985, W:   2.916140, b:  0.359022\n",
      "Epoch (       400/      1000) cost:   0.012026, W:   2.941820, b:  0.249079\n",
      "Epoch (       500/      1000) cost:   0.005788, W:   2.959636, b:  0.172804\n",
      "Epoch (       600/      1000) cost:   0.002786, W:   2.971997, b:  0.119887\n",
      "Epoch (       700/      1000) cost:   0.001341, W:   2.980572, b:  0.083174\n",
      "Epoch (       800/      1000) cost:   0.000645, W:   2.986522, b:  0.057704\n",
      "Epoch (       900/      1000) cost:   0.000311, W:   2.990649, b:  0.040033\n",
      "W:   2.993489\n",
      "b:   0.027876\n",
      "result : \n",
      "[ 3.02136459  6.01485338  9.00834216 12.00183094 14.99531973 17.98880851]\n"
     ]
    }
   ],
   "source": [
    "# 1. numpy 로 gradient descent 구현\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "xy = np.array([\n",
    "    [1.,2.,3.,4.,5.,6.,],\n",
    "    [3.,6.,9.,12.,15.,18.],\n",
    "    ])\n",
    "\n",
    "x_train = xy[0]\n",
    "y_train = xy[1]\n",
    "\n",
    "beta_gd = np.random.rand(1)\n",
    "bias = np.random.rand(1)\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_data = len(x_train)\n",
    "\n",
    "for i in range(1000):\n",
    "    hypothesis = x_train * beta_gd + bias\n",
    "    cost = np.sum( ((hypothesis - y_train)**2)/ n_data)\n",
    "    \n",
    "    gradient_w = np.sum((beta_gd * x_train - y_train + bias) * 2 * x_train) / n_data\n",
    "    gradient_b = np.sum((beta_gd * x_train - y_train + bias) * 2) / n_data\n",
    "    \n",
    "    beta_gd -= learning_rate * gradient_w\n",
    "    bias -= learning_rate * gradient_b\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print('Epoch ({:10d}/{:10d}) cost: {:10f}, W: {:10f}, b:{:10f}'.format(i, 1000, cost, float(beta_gd), float(bias)))\n",
    "        \n",
    "print('W: {:10f}'.format(float(beta_gd)))\n",
    "print('b: {:10f}'.format(float(bias)))\n",
    "print('result : ')\n",
    "print(x_train * beta_gd + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (   0/1000) cost: 86.153435, W: 1.127606, b:1.0885795\n",
      "Epoch ( 100/1000) cost: 0.195346, W: 2.7655146, b:1.0038788\n",
      "Epoch ( 200/1000) cost: 0.094024, W: 2.8373203, b:0.6964635\n",
      "Epoch ( 300/1000) cost: 0.045256, W: 2.8871374, b:0.4831872\n",
      "Epoch ( 400/1000) cost: 0.021782, W: 2.921699, b:0.335222\n",
      "Epoch ( 500/1000) cost: 0.010484, W: 2.945677, b:0.2325676\n",
      "Epoch ( 600/1000) cost: 0.005046, W: 2.962312, b:0.1613492\n",
      "Epoch ( 700/1000) cost: 0.002429, W: 2.9738533, b:0.1119396\n",
      "Epoch ( 800/1000) cost: 0.001169, W: 2.9818602, b:0.0776607\n",
      "Epoch ( 900/1000) cost: 0.000563, W: 2.9874151, b:0.0538786\n",
      "Epoch (1000/1000) cost: 0.000271, W: 2.9912691, b:0.0373794\n",
      "W:   2.991269\n",
      "b:   0.037379\n",
      "result : \n",
      "tensor([ 3.0286,  6.0199,  9.0112, 12.0025, 14.9937, 17.9850],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 2. pytorch auto_grad 로 구현\n",
    "# 참조 사이트 : https://machinelearningmastery.com/implementing-gradient-descent-in-pytorch/\n",
    "import torch\n",
    "\n",
    "x = torch.tensor([1.,2.,3.,4.,5.,6.])\n",
    "y = torch.tensor([3.,6.,9.,12.,15.,18.])\n",
    "\n",
    "w = torch.rand(1, requires_grad=True) # computation_graph 를 생성하기 위해서\n",
    "b = torch.rand(1, requires_grad=True) # requires_grad = True 로 설정하고 \n",
    "learning_rate = 0.01                  # 설정시, 미분값을 자동적으로 저장하게 된다.\n",
    "                                      # w,b 는 leaf node 이다.              \n",
    "for epoch in range(1001):\n",
    "    y_hat = x * w + b\n",
    "    loss = ((y_hat - y)**2).mean()\n",
    "    \n",
    "    w.retain_grad() # retain_grad 를 하지 않을시 non-leaf 들은 .grad 속성이 None으로저장됨\n",
    "    b.retain_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    w = w - learning_rate * w.grad\n",
    "    b = b - learning_rate * b.grad\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch ({str(epoch).rjust(4)}/1000) cost: {round(float(loss),6)}, W: {round(float(w),7)}, b:{round(float(b),7)}')\n",
    "        \n",
    "print('W: {:10f}'.format(float(w)))\n",
    "print('b: {:10f}'.format(float(b)))\n",
    "print('result : ')\n",
    "print(x * w + b)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model : MyNeuralNetwork(\n",
      "  (layer): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "weight : Parameter containing:\n",
      "tensor([[0.6738]], requires_grad=True)\n",
      "bias : Parameter containing:\n",
      "tensor([-0.4946], requires_grad=True)\n",
      "Epoch (   0/1000) cost: 90.370064, W: 1.4140239, b:-0.3218891\n",
      "Epoch ( 100/1000) cost: 0.000198, W: 2.9925377, b:0.0319481\n",
      "Epoch ( 200/1000) cost: 9.5e-05, W: 2.9948227, b:0.0221648\n",
      "Epoch ( 300/1000) cost: 4.6e-05, W: 2.996408, b:0.0153775\n",
      "Epoch ( 400/1000) cost: 2.2e-05, W: 2.997508, b:0.0106685\n",
      "Epoch ( 500/1000) cost: 1.1e-05, W: 2.998271, b:0.0074019\n",
      "Epoch ( 600/1000) cost: 5e-06, W: 2.9988005, b:0.0051356\n",
      "Epoch ( 700/1000) cost: 2e-06, W: 2.9991677, b:0.0035634\n",
      "Epoch ( 800/1000) cost: 1e-06, W: 2.9994223, b:0.0024726\n",
      "Epoch ( 900/1000) cost: 1e-06, W: 2.999599, b:0.0017157\n",
      "Epoch (1000/1000) cost: 0.0, W: 2.9997218, b:0.0011905\n",
      "W:   2.999722\n",
      "b:   0.001190\n",
      "result : \n",
      "tensor([[ 3.0009],\n",
      "        [ 6.0006],\n",
      "        [ 9.0004],\n",
      "        [12.0001],\n",
      "        [14.9998],\n",
      "        [17.9995]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 3. Neural Network 로 gradient_descent 구현\n",
    "\n",
    "# 클래스 정의하기\n",
    "# 신경망 모델을 nn.Module 의 하위클래스로 정의하고,\n",
    "# __init__ 에서 신경망 계층들을 초기화합니다. \n",
    "# nn.Module 을 상속받은 모든 클래스는 forward 메소드에\n",
    "# 입력 데이터에 대한 연산들을 구현합니다.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 신경망 정의\n",
    "class MyNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNeuralNetwork, self).__init__()\n",
    "        self.layer=nn.Linear(in_features=1, out_features=1, bias=True)\n",
    "        \n",
    "        # 아래 네줄은 써도 상관없고 안써도 상관없다. 같은 결과를 내뱉음 \n",
    "        # weight = torch.rand(1).view(1,1) # 2d matrix 로 초기화\n",
    "        # bias = torch.rand(1).view(1,1)\n",
    "        # self.layer.weight = nn.Parameter(weight)\n",
    "        # self.layer.bias = nn.Parameter(bias)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = self.layer(input)\n",
    "        return output\n",
    "    \n",
    "model = MyNeuralNetwork().to(\"cpu\")\n",
    "\n",
    "print(f\"model : {model}\")\n",
    "print(f\"weight : {model.layer.weight}\")\n",
    "print(f\"bias : {model.layer.bias}\")\n",
    "\n",
    "input = torch.tensor([1.,2.,3.,4.,5.,6.,]).view(-1,1)\n",
    "y = torch.tensor([3.,6.,9.,12.,15.,18.]).view(-1,1)\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.01)\n",
    "\n",
    "epochs = 1001\n",
    "loss_fn = nn.MSELoss()\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_fn(model(input), y)\n",
    "    loss.backward()  # backward : w, b에 대한 기울기 진행\n",
    "    optimizer.step() # model.parameters() 에서 리턴되는 변수들의 기울기에\n",
    "                     # 학습률 0.01 을 곱해서 빼준뒤에 업데이트 한다.\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch ({str(epoch).rjust(4)}/1000) cost: {round(float(loss),6)}, W: {round(float(model.layer.weight),7)}, b:{round(float(model.layer.bias),7)}')\n",
    "\n",
    "print('W: {:10f}'.format(float(model.layer.weight)))\n",
    "print('b: {:10f}'.format(float(model.layer.bias)))\n",
    "print('result : ')\n",
    "print(model(input))    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
