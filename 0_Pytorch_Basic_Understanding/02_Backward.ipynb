{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x000001FF5FBEA8E0>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_27228\\764545402.py:6: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  info.append(f'{name}({getattr(tensor, name, None)})')\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_27228\\764545402.py:19: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  print(y.grad)\n"
     ]
    }
   ],
   "source": [
    "# https://teamdable.github.io/techblog/PyTorch-Autograd#Code-10\n",
    "# 생각보다 한국어로 되어있는 데 도움이 많이 된다. \n",
    "import torch\n",
    "\n",
    "def get_tensor_info(tensor):\n",
    "  info = []\n",
    "  for name in ['requires_grad', 'is_leaf', 'retains_grad', 'grad_fn', 'grad']:\n",
    "    info.append(f'{name}({getattr(tensor, name, None)})')\n",
    "  info.append(f'tensor({str(tensor)})')\n",
    "  return ' '.join(info)\n",
    "\n",
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "y = x ** 3\n",
    "z = torch.log(y)\n",
    "\n",
    "# print('x', get_tensor_info(x))\n",
    "print('y', get_tensor_info(y))\n",
    "# print('z', get_tensor_info(z))\n",
    "\n",
    "z.backward()\n",
    "print(y.grad)\n",
    "# print('x_after_backward', get_tensor_info(x))\n",
    "# print('y_after_backward', get_tensor_info(y))\n",
    "# print('z_after_backward', get_tensor_info(z))\n",
    "\n",
    "# Gradient를 계산하더라도 그 Gradient를 항상 저장하지는 않습니다. \n",
    "# Tensor의 is_leaf가 True이고 requires_grad가 True인 경우에만 \n",
    "# Gradient를 계산하고 grad에 Gradient를 저장합니다.\n",
    "\n",
    "# 머 대충 계산은 하더라도 저장은 하기 싫다 ... 이런 의미로 알아들으면 될듯\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(None) tensor(tensor(5., requires_grad=True))\n",
      "y requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x00000292F894A9D0>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
      "z requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<LogBackward0 object at 0x00000292F7E5D6A0>) grad(None) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n",
      "\n",
      "x_after_backward requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(0.6000000238418579) tensor(tensor(5., requires_grad=True))\n",
      "y_after_backward requires_grad(True) is_leaf(False) retains_grad(True) grad_fn(<PowBackward0 object at 0x00000292F7E5D6A0>) grad(0.00800000037997961) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
      "z_after_backward requires_grad(True) is_leaf(False) retains_grad(True) grad_fn(<LogBackward0 object at 0x00000292F8073F40>) grad(1.0) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4596\\850759442.py:6: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  info.append(f'{name}({getattr(tensor, name, None)})')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_tensor_info(tensor):\n",
    "  info = []\n",
    "  for name in ['requires_grad', 'is_leaf', 'retains_grad', 'grad_fn', 'grad']:\n",
    "    info.append(f'{name}({getattr(tensor, name, None)})')\n",
    "  info.append(f'tensor({str(tensor)})')\n",
    "  return ' '.join(info)\n",
    "\n",
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "y = x ** 3\n",
    "z = torch.log(y)\n",
    "\n",
    "print('x', get_tensor_info(x))\n",
    "print('y', get_tensor_info(y))\n",
    "print('z', get_tensor_info(z))\n",
    "\n",
    "y.retain_grad()\n",
    "z.retain_grad()\n",
    "z.backward()\n",
    "\n",
    "print()\n",
    "print('x_after_backward', get_tensor_info(x))\n",
    "print('y_after_backward', get_tensor_info(y))\n",
    "print('z_after_backward', get_tensor_info(z))\n",
    "\n",
    "# 만약에 is_leaf가 아닌 Tensor에 Gradient가 grad에 저장되게 하고 싶으면, \n",
    "# 해당 Tensor의 retain_grad()를 호출해서 retains_grad를 True로 설정하고,\n",
    "# backward()를 호출하면 grad에 저장됩니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(None) tensor(tensor(5., requires_grad=True))\n",
      "y requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x000001FF5F290520>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
      "z requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<LogBackward0 object at 0x000001FF5F290190>) grad(None) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n",
      "x_after_backward requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(0.6000000238418579) tensor(tensor(5., requires_grad=True))\n",
      "y_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x000001FF5F290790>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
      "z_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<LogBackward0 object at 0x000001FF5F290EE0>) grad(None) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_27228\\2162632940.py:6: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  info.append(f'{name}({getattr(tensor, name, None)})')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27228\\2162632940.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'z_after_backward'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_tensor_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# backward 함수는 graph 를 만든 이후에 한번 만 호출하는 것을 가정한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m             )\n\u001b[1;32m--> 488\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         )\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_tensor_info(tensor):\n",
    "  info = []\n",
    "  for name in ['requires_grad', 'is_leaf', 'retains_grad', 'grad_fn', 'grad']:\n",
    "    info.append(f'{name}({getattr(tensor, name, None)})')\n",
    "  info.append(f'tensor({str(tensor)})')\n",
    "  return ' '.join(info)\n",
    "\n",
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "y = x ** 3\n",
    "z = torch.log(y)\n",
    "\n",
    "print('x', get_tensor_info(x))\n",
    "print('y', get_tensor_info(y))\n",
    "print('z', get_tensor_info(z))\n",
    "\n",
    "z.backward()\n",
    "\n",
    "print('x_after_backward', get_tensor_info(x))\n",
    "print('y_after_backward', get_tensor_info(y))\n",
    "print('z_after_backward', get_tensor_info(z))\n",
    "\n",
    "z.backward()\n",
    "\n",
    "# backward 함수는 graph 를 만든 이후에 한번 만 호출하는 것을 가정한다. \n",
    "# 즉, backward 함수 호출이후 backward의 실행에 필요한 각종 자원해제를 하기에\n",
    "# Exception 이 발생한다. (연속호출시...)\n",
    "\n",
    "# 버퍼가 있는데...그게 사라진다. \n",
    "# 미분값을 저장해놓고 사용하기위한 버퍼가 있는데 그게 사라진다요..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(None) tensor(tensor(5., requires_grad=True))\n",
      "y requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x000001FF5F290520>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
      "z requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<LogBackward0 object at 0x000001FF5F290340>) grad(None) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n",
      "\n",
      "x_after_backward requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(0.6000000238418579) tensor(tensor(5., requires_grad=True))\n",
      "\n",
      "x_after_2backward requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(1.2000000476837158) tensor(tensor(5., requires_grad=True))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_27228\\3806411473.py:6: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  info.append(f'{name}({getattr(tensor, name, None)})')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_tensor_info(tensor):\n",
    "  info = []\n",
    "  for name in ['requires_grad', 'is_leaf', 'retains_grad', 'grad_fn', 'grad']:\n",
    "    info.append(f'{name}({getattr(tensor, name, None)})')\n",
    "  info.append(f'tensor({str(tensor)})')\n",
    "  return ' '.join(info)\n",
    "\n",
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "y = x ** 3\n",
    "z = torch.log(y)\n",
    "\n",
    "print('x', get_tensor_info(x))\n",
    "print('y', get_tensor_info(y))\n",
    "print('z', get_tensor_info(z))\n",
    "\n",
    "z.backward(retain_graph=True)\n",
    "print()\n",
    "\n",
    "print('x_after_backward', get_tensor_info(x))\n",
    "# print('y_after_backward', get_tensor_info(y))\n",
    "# print('z_after_backward', get_tensor_info(z))\n",
    "\n",
    "print()\n",
    "z.backward()\n",
    "\n",
    "print('x_after_2backward', get_tensor_info(x))\n",
    "# print('y_after_2backward', get_tensor_info(y))\n",
    "# print('z_after_2backward', get_tensor_info(z))\n",
    "\n",
    "# 첫 번째 backward()를 호출한 뒤의 x.grad의 값과 \n",
    "# 두 번째 backward()를 호출한 뒤의 x.grad의 값을 비교해보면 값이 다르다.\n",
    "# backward()에서 Gradient를 x.grad에 저장할 때 \n",
    "# Gradient를 덮어쓰는 것이 아니라 기존 저장되어 있는 x.grad값에 \n",
    "# Gradient를 더하기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "x requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(None) tensor(tensor(5., requires_grad=True))\n",
      "y requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x000001FF5F2AA220>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
      "z requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<LogBackward0 object at 0x000001FF5DDDC4F0>) grad(None) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n",
      "\n",
      "x_after_backward requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(0.6000000238418579) tensor(tensor(5., requires_grad=True))\n",
      "y_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x000001FF5DDDC4F0>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
      "z_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<LogBackward0 object at 0x000001FF5F2AA190>) grad(None) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n",
      "\n",
      "x_after_2backward requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(0.6000000238418579) tensor(tensor(5., requires_grad=True))\n",
      "y_after_2backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x000001FF651CA6D0>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
      "z_after_2backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<LogBackward0 object at 0x000001FF5F2AA190>) grad(None) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_27228\\1013391943.py:6: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  info.append(f'{name}({getattr(tensor, name, None)})')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_tensor_info(tensor):\n",
    "  info = []\n",
    "  for name in ['requires_grad', 'is_leaf', 'retains_grad', 'grad_fn', 'grad']:\n",
    "    info.append(f'{name}({getattr(tensor, name, None)})')\n",
    "  info.append(f'tensor({str(tensor)})')\n",
    "  return ' '.join(info)\n",
    "\n",
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "y = x ** 3\n",
    "z = torch.log(y)\n",
    "\n",
    "print()\n",
    "print('x', get_tensor_info(x))\n",
    "print('y', get_tensor_info(y))\n",
    "print('z', get_tensor_info(z))\n",
    "\n",
    "z.backward(retain_graph=True)\n",
    "\n",
    "print()\n",
    "print('x_after_backward', get_tensor_info(x))\n",
    "print('y_after_backward', get_tensor_info(y))\n",
    "print('z_after_backward', get_tensor_info(z))\n",
    "\n",
    "x.grad.zero_()\n",
    "z.backward()\n",
    "\n",
    "print()\n",
    "print('x_after_2backward', get_tensor_info(x))\n",
    "print('y_after_2backward', get_tensor_info(y))\n",
    "print('z_after_2backward', get_tensor_info(z))\n",
    "\n",
    "# retain_graph = True + x_grad.zero_\n",
    "# 를 통하여 grad 를 0 으로 초기화했기에 \n",
    "\n",
    "# 물론 의문은 든다 도데체 왜 축적을 시키는 거고 \n",
    "# 축적을 시키는 목적은 무엇일까?\n",
    "\n",
    "# 암튼, x.grad 를 backward 를 호출하기 전에 초기화해주었기때문에\n",
    "# 첫번쨰 backward를 호출한 후의 x.grad 값과 두번째 backward() 를 호출한 후의\n",
    "# x.grad 값은 같다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(None) tensor(tensor(5., requires_grad=True))\n",
      "y requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x00000292FD44DAF0>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
      "w requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x00000292FD44DD00>) grad(None) tensor(tensor(25., grad_fn=<PowBackward0>))\n",
      "z requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<AddBackward0 object at 0x00000292FD44D7C0>) grad(None) tensor(tensor(9.8283, grad_fn=<AddBackward0>))\n",
      "\n",
      "x_after_backward requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(1.600000023841858) tensor(tensor(5., requires_grad=True))\n",
      "y_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x00000292FD44DA30>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
      "w_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x00000292FD44DD30>) grad(None) tensor(tensor(25., grad_fn=<PowBackward0>))\n",
      "z_after_backward requires_grad(True) is_leaf(False) retains_grad(True) grad_fn(<AddBackward0 object at 0x00000292FD44DA30>) grad(1.0) tensor(tensor(9.8283, grad_fn=<AddBackward0>))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4596\\3344834892.py:6: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  info.append(f'{name}({getattr(tensor, name, None)})')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_tensor_info(tensor):\n",
    "  info = []\n",
    "  for name in ['requires_grad', 'is_leaf', 'retains_grad', 'grad_fn', 'grad']:\n",
    "    info.append(f'{name}({getattr(tensor, name, None)})')\n",
    "  info.append(f'tensor({str(tensor)})')\n",
    "  return ' '.join(info)\n",
    "\n",
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "y = x ** 3\n",
    "w = x ** 2\n",
    "z = torch.log(y) + torch.sqrt(w)\n",
    "\n",
    "print('x', get_tensor_info(x))\n",
    "print('y', get_tensor_info(y))\n",
    "print('w', get_tensor_info(w))\n",
    "print('z', get_tensor_info(z))\n",
    "\n",
    "\n",
    "z.retain_grad()\n",
    "z.backward()\n",
    "print()\n",
    "\n",
    "\n",
    "print('x_after_backward', get_tensor_info(x))\n",
    "print('y_after_backward', get_tensor_info(y))\n",
    "print('w_after_backward', get_tensor_info(w))\n",
    "print('z_after_backward', get_tensor_info(z))\n",
    "\n",
    "# 그냥 머 더하기식의 역전파의 경우 각각의 미분값이 더해져서 \n",
    "# 하나의 값이 된다 이런느낌임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
